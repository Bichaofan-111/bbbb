import pandas as pd
import torch
import gc
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import roc_auc_score
import os
import time
import numpy as np
from scipy.special import expit
import shutil

# --- 1. å®šä¹‰å®éªŒé…ç½® ---
experiments = [
    {"learning_rate": 2e-5, "per_device_train_batch_size": 16, "weight_decay": 0.01, "note": "Baseline"},
    {"learning_rate": 5e-5, "per_device_train_batch_size": 16, "weight_decay": 0.01, "note": "High LR"},
    {"learning_rate": 2e-5, "per_device_train_batch_size": 32, "weight_decay": 0.01, "note": "High BS"},
    {"learning_rate": 5e-5, "per_device_train_batch_size": 32, "weight_decay": 0.01, "note": "High BS & LR"}
]

REPETITIONS = 5
OUTPUT_FILE = "doe_fast_results.csv"
DATA_FILE = "train.csv"

# --- 2. å‡†å¤‡æ•°æ® ---
if not os.path.exists(DATA_FILE):
    print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {DATA_FILE}")
    exit()
else:
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    # ä¿æŒ 5% æ•°æ®é‡ï¼Œå¦‚æœè§‰å¾—å¤ªæ…¢ï¼Œå¯ä»¥æŠŠ frac=0.05 æ”¹æˆ frac=0.01 (1%)
    raw_df = pd.read_csv(DATA_FILE).sample(frac=0.05, random_state=42)

print(f"ğŸ”¥ æé€Ÿæ¨¡å¼å·²å¼€å¯ï¼šæœ¬æ¬¡å®éªŒä»…ä½¿ç”¨ {len(raw_df)} æ¡æ•°æ®")

labels = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")


def preprocess(examples):
    return tokenizer(examples["comment_text"], padding="max_length", truncation=True, max_length=128)


def format_labels(examples):
    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶è½¬æ¢ä¸º float32
    labels_matrix = np.array([examples[l] for l in labels], dtype=np.float32).T
    return {"labels": labels_matrix.tolist()}


try:
    ds = Dataset.from_pandas(raw_df)
    ds = ds.map(preprocess, batched=True)
    ds = ds.map(format_labels, batched=True)

    cols_to_keep = ['input_ids', 'attention_mask', 'labels']
    ds = ds.remove_columns([c for c in ds.column_names if c not in cols_to_keep])
    ds.set_format("torch")

    # æ‰“å°æ£€æŸ¥
    print(f"Label ç±»å‹æ£€æŸ¥: {ds[0]['labels'].dtype} (å¿…é¡»æ˜¯ float32)")

    ds = ds.train_test_split(test_size=0.1, seed=42)
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆã€‚")
except Exception as e:
    print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
    exit()


# --- 3. è¾…åŠ©å‡½æ•° ---
def compute_metrics(p):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    probs = expit(preds)
    roc_auc = roc_auc_score(p.label_ids, probs, average="micro")
    return {"roc_auc": roc_auc}


def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()


# --- 4. å¼€å§‹å¾ªç¯è¿è¡Œ ---
results = []
total_runs = len(experiments) * REPETITIONS
current_run = 0
start_time_all = time.time()

print(f"ğŸ å¼€å§‹ DOE å®éªŒï¼Œæ€»è®¡ {total_runs} æ¬¡è¿è¡Œ...")

for i, params in enumerate(experiments):
    for rep in range(REPETITIONS):
        current_run += 1
        run_name = f"{params['note']}_Rep{rep + 1}"
        output_dir = f"./doe_temp/run_{current_run}"

        print(
            f"\n[{current_run}/{total_runs}] ğŸš€ è¿è¡Œ: {run_name} | LR={params['learning_rate']} | BS={params['per_device_train_batch_size']}")

        model = RobertaForSequenceClassification.from_pretrained(
            "roberta-base",
            num_labels=6,
            problem_type="multi_label_classification"
        )
        if torch.cuda.is_available():
            model.to('cuda')

        args = TrainingArguments(
            output_dir=output_dir,
            learning_rate=params["learning_rate"],
            per_device_train_batch_size=params["per_device_train_batch_size"],
            per_device_eval_batch_size=32,
            weight_decay=params["weight_decay"],
            num_train_epochs=3,

            # --- å…³é”®ä¿®æ”¹ï¼šå¼€å¯å¯è§†åŒ– ---
            disable_tqdm=False,  # âœ… å¼€å¯è¿›åº¦æ¡
            logging_steps=50,  # âœ… æ¯ 50 æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—
            eval_strategy="epoch",

            save_strategy="no",
            fp16=torch.cuda.is_available(),
            seed=42 + rep,
            report_to="none"
        )

        trainer = Trainer(
            model=model,
            args=args,
            train_dataset=ds["train"],
            eval_dataset=ds["test"],
            processing_class=tokenizer,
            compute_metrics=compute_metrics
        )

        try:
            trainer.train()
            eval_res = trainer.evaluate()
            auc = eval_res["eval_roc_auc"]
            print(f"   âœ… å®Œæˆ -> ROC-AUC: {auc:.4f}")

            record = params.copy()
            record["repetition"] = rep + 1
            record["roc_auc"] = auc
            results.append(record)
            pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False)

        except Exception as e:
            print(f"   âŒ è®­ç»ƒå‡ºé”™: {e}")
            import traceback

            traceback.print_exc()

        finally:
            del model
            del trainer
            cleanup_gpu()
            if os.path.exists(output_dir):
                try:
                    shutil.rmtree(output_dir, ignore_errors=True)
                except:
                    pass

total_time = (time.time() - start_time_all) / 60
print(f"\nğŸ‰ æ‰€æœ‰å®éªŒç»“æŸï¼æ€»è€—æ—¶: {total_time:.1f} åˆ†é’Ÿ")
print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_FILE}")